# ç†è§£æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

import torch
import torch.nn as nn
import torch.optim as optim

# 1. å®šä¹‰æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
model = nn.Linear(1, 1)                # ç®€å•çº¿æ€§æ¨¡å‹ y = wx + b
criterion = nn.MSELoss()               # å›å½’ä»»åŠ¡ç”¨ MSE
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 2. å‡†å¤‡æ•°æ®
x = torch.tensor([[2.0]])              # è¾“å…¥ x=2
y_true = torch.tensor([[5.0]])         # çœŸå® y=5ï¼ˆå‡è®¾ y = 2x + 1ï¼‰

# æ¢¯åº¦ Gradient
# 3. è®­ç»ƒä¸€æ­¥
optimizer.zero_grad()                  # ğŸ”‘ æ¸…ç©ºä¸Šæ¬¡æ¢¯åº¦ï¼ˆé‡è¦ï¼ï¼‰

y_pred = model(x)                      # å‰å‘ä¼ æ’­ â†’ æ¯”å¦‚å¾—åˆ° y=3.0
loss = criterion(y_pred, y_true)       # è®¡ç®— loss = (5-3)^2 = 4

loss.backward()                        # ğŸ”‘ åå‘ä¼ æ’­ï¼šè®¡ç®— d_loss/d_w, d_loss/d_b
print("æ¢¯åº¦:", model.weight.grad)      # æ¯”å¦‚ tensor([[-4.]])

optimizer.step()                       # ğŸ”‘ æ›´æ–°å‚æ•°ï¼šw = w - lr * grad
print("æ›´æ–°åæƒé‡:", model.weight)     # æ¯”å¦‚ä» 1.0 â†’ 1.04